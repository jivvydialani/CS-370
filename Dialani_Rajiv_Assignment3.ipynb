{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Augmenting training set images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 105s 3ms/step - loss: 1.7639 - accuracy: 0.3625 - val_loss: 1.4029 - val_accuracy: 0.4915\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 108s 3ms/step - loss: 1.2962 - accuracy: 0.5401 - val_loss: 1.3316 - val_accuracy: 0.5409\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 102s 3ms/step - loss: 1.0940 - accuracy: 0.6159 - val_loss: 0.9571 - val_accuracy: 0.6740\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 0.9666 - accuracy: 0.6646 - val_loss: 0.8849 - val_accuracy: 0.6939\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.8751 - accuracy: 0.6948 - val_loss: 0.8461 - val_accuracy: 0.7149\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.8076 - accuracy: 0.7194 - val_loss: 0.7512 - val_accuracy: 0.7454\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 160s 4ms/step - loss: 0.7452 - accuracy: 0.7416 - val_loss: 0.8490 - val_accuracy: 0.7179\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.6975 - accuracy: 0.7578 - val_loss: 0.7872 - val_accuracy: 0.7377\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.6566 - accuracy: 0.7732 - val_loss: 0.7908 - val_accuracy: 0.7479\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.6245 - accuracy: 0.7849 - val_loss: 0.7155 - val_accuracy: 0.7625\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.6009 - accuracy: 0.7930 - val_loss: 0.7438 - val_accuracy: 0.7659\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.5739 - accuracy: 0.8028 - val_loss: 0.7775 - val_accuracy: 0.7419\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.5642 - accuracy: 0.8057 - val_loss: 0.7378 - val_accuracy: 0.7522\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.5456 - accuracy: 0.8135 - val_loss: 0.7217 - val_accuracy: 0.7801\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.5405 - accuracy: 0.8157 - val_loss: 0.7616 - val_accuracy: 0.7807\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.5309 - accuracy: 0.8216 - val_loss: 0.7873 - val_accuracy: 0.7706\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.5228 - accuracy: 0.8232 - val_loss: 0.6562 - val_accuracy: 0.7867\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.5242 - accuracy: 0.8246 - val_loss: 0.7832 - val_accuracy: 0.7529\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.5198 - accuracy: 0.8264 - val_loss: 0.6917 - val_accuracy: 0.7806\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.5244 - accuracy: 0.8246 - val_loss: 0.7708 - val_accuracy: 0.7886\n",
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "Test score: 0.795948212814331\n",
      "Test accuracy: 0.7804999947547913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=20, verbose=1, steps_per_epoch=390)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 1.4448 - accuracy: 0.5036\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 186s 476ms/step - loss: 1.3452 - accuracy: 0.5333\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 192s 493ms/step - loss: 1.3112 - accuracy: 0.5466\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 187s 479ms/step - loss: 1.2832 - accuracy: 0.5568\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 154s 395ms/step - loss: 1.2697 - accuracy: 0.5603\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 113s 290ms/step - loss: 1.2603 - accuracy: 0.5653\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 117s 300ms/step - loss: 1.2398 - accuracy: 0.5730\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 112s 288ms/step - loss: 1.2350 - accuracy: 0.5746\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 114s 291ms/step - loss: 1.2283 - accuracy: 0.5798\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 115s 295ms/step - loss: 1.2265 - accuracy: 0.5766\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 112s 286ms/step - loss: 1.2240 - accuracy: 0.5806\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 118s 302ms/step - loss: 1.2199 - accuracy: 0.5832\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 111s 285ms/step - loss: 1.2264 - accuracy: 0.5810\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 112s 286ms/step - loss: 1.2221 - accuracy: 0.5822\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 118s 303ms/step - loss: 1.2230 - accuracy: 0.5813\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 112s 287ms/step - loss: 1.2199 - accuracy: 0.5842\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 116s 297ms/step - loss: 1.2274 - accuracy: 0.5825\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 114s 292ms/step - loss: 1.2238 - accuracy: 0.5833\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 113s 290ms/step - loss: 1.2266 - accuracy: 0.5823\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 120s 308ms/step - loss: 1.2260 - accuracy: 0.5846\n",
      "10000/10000 [==============================] - 9s 942us/step\n",
      "Test score: 1.2549148635864258\n",
      "Test accuracy: 0.5746999979019165\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context # Used to bypass SSL certificate verification\n",
    "os.makedirs('preview', exist_ok=True)# Make directory for augmentation of images\n",
    "NUM_TO_AUGMENT=5\n",
    "\n",
    "#CIFAR_10 is a set  of 60K images 32x32 pixels on Flatten 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "#augementing\n",
    "print(\"Augmenting training set images...\")\n",
    "# convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "#save model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "#And the weights learned by our deep network on the training set\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "datagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\n",
    "for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "    if num_aug >= NUM_TO_AUGMENT:\n",
    "        break\n",
    "    xtas.append(x_aug[0])\n",
    "    num_aug += 1\n",
    "#fit the datagen\n",
    "datagen.fit(X_train)\n",
    "#train\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0], epochs=NB_EPOCH, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical and Privacy Implications\n",
    "\n",
    "The algorithm I just created can distinguish more realistic images between animals or vehicles. While this doesn’t pose a serious threat or ethical dilemma, the algorithm can be trained on different types of images. I’d consider my face to be  “Personally Identifiable Information (PII), which [can] uniquely [identify] a person” (Dorschel, 2019). The algorithm's ability to distinguish between more realistic images raises concerns about its potential use in facial recognition technology. If trained on a dataset containing images of people's faces, the algorithm could potentially be used for facial recognition purposes. This raises significant ethical and privacy implications. \n",
    "\n",
    "In fact, there is a company, called Clearview AI, that uses facial recognition software. “You take a picture of a person, upload it and get to see public photos of that person, along with links to where those photos appeared” (Hill, 2020). Clearview’s system has a database of more than 3 billion images that were “scraped from Facebook, YouTube, Venmo and millions of other websites” (Hill, 2020). Scraping images from certain websites, like Facebook, is a violation of some websites’ terms of service, which is unethical and may also be illegal. This is why transparency is crucial in Machine Learning algorithms. Transparent algorithms ensure that developers and users are aware of how data is collected, used, and processed. This transparency helps ensure that AI systems adhere to ethical guidelines and respect the rights and privacy of individuals. The widespread deployment of facial recognition algorithms could result in increased surveillance and control by governments, corporations, or other entities. This could threaten civil liberties, freedom of expression, and democratic principles. \n",
    "\n",
    "“Absent a very strong privacy law, we’re all screwed” (Hill, 2020).\n",
    "\n",
    "#### References:\n",
    "\n",
    "\n",
    "Dorschel, A. (2019, April 24). Rethinking Data Privacy: Impact of Machine Learning. Medium. https://medium.com/luminovo/data-privacy-in-machine-learning-a-technical-deep-dive-f7f0365b1d60 \n",
    "\n",
    "\n",
    "\n",
    "Hill, K. (2020, January 21). The Secretive Company That Might End Privacy as We Know It. International New York Times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
